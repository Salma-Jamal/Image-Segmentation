{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50e4ad62-7f04-420e-9c66-11e13a879aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.49.0+computecanada\n",
    "# !pip install torch==2.5.0\n",
    "# !pip install torchvision==0.20.0+computecanada\n",
    "# !pip install lightning\n",
    "# !pip install sklearn\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b5e4ea-f2e5-4794-9f28-13b6f5f21ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcf39a-f915-47f4-bf4e-0de990a3c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ccdcdf-c6ad-450c-878c-acc75a253291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, dropout_p=0.5, embedding_size=128, freeze=False, linear_eval=False):\n",
    "        super().__init__()\n",
    "        self.linear_eval = linear_eval\n",
    "        self.dropout_p = dropout_p\n",
    "        self.embedding_size = embedding_size\n",
    "        self.encoder = Dinov2Model.from_pretrained('microsoft/rad-dino')\n",
    "        \n",
    "        if freeze:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.Dropout(p=self.dropout_p),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.linear_eval:\n",
    "            x = torch.cat(x, dim=0)\n",
    "\n",
    "\n",
    "        outputs = self.encoder(x)\n",
    "        encoding = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        projection = self.projection(encoding)\n",
    "\n",
    "        return projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c2eeb2-c155-4dbd-b698-d8f393a217e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1414065/985197841.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  projection_state_dict = torch.load(retrieval_checkpoint+'projection_head.pth', map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_checkpoint = '/home/saahmed/scratch/projects/Image-segmentation/retrieval/checkpoints/simclr_rad-dino_pos-pairs_aug-pairs_epochs100/best_model/'\n",
    "\n",
    "retrieval_model = SimCLR(dropout_p=0.3, embedding_size=128, freeze=False, linear_eval=True)\n",
    "\n",
    "encoder_state_dict = load_file(retrieval_checkpoint+'model.safetensors')\n",
    "retrieval_model.encoder.load_state_dict(encoder_state_dict)\n",
    "\n",
    "projection_state_dict = torch.load(retrieval_checkpoint+'projection_head.pth', map_location=torch.device('cpu'))\n",
    "retrieval_model.projection.load_state_dict(projection_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e13999-a13a-453c-b10d-b067e201150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d3018f1-5bf6-483b-98fb-a77fb1fc6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_rgb(img):\n",
    "#     return img.convert(\"RGB\")\n",
    "    \n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.Lambda(convert_to_rgb),\n",
    "#     transforms.ToTensor(),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd0172f2-5c1d-46b2-8a7b-4455eb86d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_img_embeddings(image_path,model):\n",
    "#     model.eval()\n",
    "#     img = Image.open(image_path)\n",
    "#     img_tensor = preprocess(img)\n",
    "    \n",
    "#     img_tensor = img_tensor.unsqueeze(0)\n",
    "#     with torch.no_grad():\n",
    "#         embedding = model(img_tensor)\n",
    "    \n",
    "#     return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa6812d9-aa04-4f7e-8972-d9de0a9f2d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = '/home/saahmed/scratch/projects/Image-segmentation/datasets/ACDC/processed_data/Training/ed/images/patient001_frame01_slice_0.png'\n",
    "# embedding1 = get_img_embeddings(img_path,retrieval_model)\n",
    "# print(embedding1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1523f847-11a2-4d11-b1aa-609c5d07e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = '/home/saahmed/scratch/projects/Image-segmentation/datasets/ACDC/processed_data/Training/ed/images/patient001_frame01_slice_1.png'\n",
    "# embedding2 = get_img_embeddings(img_path,retrieval_model)\n",
    "# print(embedding2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87345c11-85db-4b4f-af13-f20a231bc7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity = F.cosine_similarity(embedding1, embedding2, dim=1)\n",
    "# similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16340da0-0741-420e-a037-40c1bc00b99c",
   "metadata": {},
   "source": [
    "# Code Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b20791-deb5-4169-ba1c-7a3f35038651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that your segmentation model is Segformer.\n",
    "# Since the composite input will have more channels (e.g., query (3) + 2 guide images (3 each) + 2 masks (1 each) = 11 channels),\n",
    "# we wrap the segformer with an input adapter to map the 11 channels to 3 channels.\n",
    "class JointSegmentationModel(nn.Module):\n",
    "    def __init__(self, base_model, composite_in_channels=11):\n",
    "        super().__init__()\n",
    "        self.input_adapter = nn.Conv2d(composite_in_channels, 3, kernel_size=1)  # maps 11 channels to 3 channels\n",
    "        self.base_model = base_model  # e.g., a SegformerForSemanticSegmentation instance\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, composite_in_channels, H, W)\n",
    "        x = self.input_adapter(x)\n",
    "        outputs = self.base_model(pixel_values=x, return_dict=True)\n",
    "        # Upsample logits to match the input resolution\n",
    "        logits = F.interpolate(outputs[\"logits\"], size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        return logits\n",
    "\n",
    "# ----------------------------\n",
    "# Loss Function\n",
    "# ----------------------------\n",
    "\n",
    "def dice_coef_loss(predictions, ground_truths, num_classes=4, dims=(1, 2), smooth=1e-8):\n",
    "    \"\"\"\n",
    "    Computes a combined Dice coefficient and Cross-Entropy loss.\n",
    "    predictions: (B, num_classes, H, W)\n",
    "    ground_truths: (B, H, W) with integer labels in [0, num_classes-1]\n",
    "    \"\"\"\n",
    "    ground_truth_oh = F.one_hot(ground_truths, num_classes=num_classes)  # (B, H, W, num_classes)\n",
    "    prediction_norm = F.softmax(predictions, dim=1).permute(0, 2, 3, 1)    # (B, H, W, num_classes)\n",
    "    intersection = (prediction_norm * ground_truth_oh).sum(dim=dims)\n",
    "    summation = prediction_norm.sum(dim=dims) + ground_truth_oh.sum(dim=dims)\n",
    "    dice = (2.0 * intersection + smooth) / (summation + smooth)\n",
    "    dice_mean = dice.mean()\n",
    "    CE = F.cross_entropy(predictions, ground_truths)\n",
    "    return (1.0 - dice_mean) + CE\n",
    "\n",
    "# ----------------------------\n",
    "# Differentiable Top-2 Retrieval\n",
    "# ----------------------------\n",
    "\n",
    "def differentiable_top2(similarity, tau=1.0):\n",
    "    \"\"\"\n",
    "    Given similarity scores of shape (B, N), returns two one-hot weight vectors (B, N)\n",
    "    using Gumbel-softmax to approximate the top-2 selections in a differentiable manner.\n",
    "    \"\"\"\n",
    "    weights1 = F.gumbel_softmax(similarity, tau=tau, hard=True)  # first selection (one-hot)\n",
    "    B, N = similarity.shape\n",
    "    masked_similarity = similarity.clone()\n",
    "    # For each batch element, mask the index selected in weights1\n",
    "    indices = weights1.argmax(dim=-1, keepdim=True)  # shape (B, 1)\n",
    "    for i in range(B):\n",
    "        masked_similarity[i, indices[i]] = float('-inf')\n",
    "    weights2 = F.gumbel_softmax(masked_similarity, tau=tau, hard=True)  # second selection\n",
    "    return weights1, weights2\n",
    "\n",
    "# ----------------------------\n",
    "# Joint Training Lightning Module\n",
    "# ----------------------------\n",
    "\n",
    "class JointTrainingModule(pl.LightningModule):\n",
    "    def __init__(self, retrieval_model, segmentation_model ,num_classes=4, lr=1e-4):\n",
    "        \"\"\"\n",
    "        retrieval_model: instance of SimCLR.\n",
    "        segmentation_model: instance of JointSegmentationModel.\n",
    "        num_classes: number of segmentation classes.\n",
    "        lr: learning rate.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.retrieval_model = retrieval_model\n",
    "        self.segmentation_model = segmentation_model\n",
    "        self.lr = lr\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Initializing the required metric objects.\n",
    "        self.mean_train_loss = MeanMetric()\n",
    "        self.mean_valid_loss = MeanMetric()\n",
    "       \n",
    " \n",
    "\n",
    "    def forward(self, query_image, gallery_images, gallery_masks):\n",
    "        \"\"\"\n",
    "        Retrieves two guide images (and masks) for the query image.\n",
    "        query_image: (B, 3, H, W)\n",
    "        gallery_images: (B, N, 3, H, W) for each query.\n",
    "        gallery_masks: (B, N, H, W) for each query.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute query embedding\n",
    "        query_embedding = self.retrieval_model(query_image)  # (B, emb_dim)\n",
    "\n",
    "        # Process gallery images: flatten gallery dimension to combine batch and candidate indices\n",
    "        B, N, C, H, W = gallery_images.shape\n",
    "        gallery_images_flat = gallery_images.view(B * N, C, H, W)\n",
    "\n",
    "\n",
    "        tempbatch_size = 2\n",
    "        gallery_embeddings_list = []\n",
    "    \n",
    "        def compute_embedding(x):\n",
    "            return self.retrieval_model(x)\n",
    "    \n",
    "        for i in range(0, B * N, tempbatch_size):\n",
    "            batch = gallery_images_flat[i:i + tempbatch_size]\n",
    "            emb = torch_checkpoint(compute_embedding, batch, use_reentrant=False)\n",
    "            gallery_embeddings_list.append(emb)\n",
    "        gallery_embeddings_flat = torch.cat(gallery_embeddings_list, dim=0)\n",
    "\n",
    "        # gallery_embeddings_flat = self.retrieval_model(gallery_images_flat)  # (B*N, emb_dim)\n",
    "        emb_dim = gallery_embeddings_flat.shape[1]\n",
    "        gallery_embeddings = gallery_embeddings_flat.view(B, N, emb_dim)  # (B, N, emb_dim)\n",
    "\n",
    "\n",
    "        similarity = F.cosine_similarity(query_embedding.unsqueeze(1),  gallery_embeddings, dim=-1)  # (B, N)\n",
    "        print(\"calculated similarity\")\n",
    "        # Obtain two guide selections using differentiable top-2\n",
    "        weights1, weights2 = differentiable_top2(similarity, tau=1.0)  # each of shape (B, N)\n",
    "        print(\"differentiable_top2\")\n",
    "        # Retrieve guide images and masks via weighted sum (will be one-hot selections due to hard=True)\n",
    "        guide_image1 = torch.einsum('bn,bnchw->bchw', weights1, gallery_images)\n",
    "        print(\"einsum\")\n",
    "        guide_image2 = torch.einsum('bn,bnchw->bchw', weights2, gallery_images)\n",
    "        print(\"einsum\")\n",
    "        guide_mask1 = torch.einsum('bn,bnhw->bhw', weights1, gallery_masks.float())\n",
    "        print(\"einsum\")\n",
    "        guide_mask2 = torch.einsum('bn,bnhw->bhw', weights2, gallery_masks.float())\n",
    "        print(\"einsum\")\n",
    "        return guide_image1, guide_image2, guide_mask1, guide_mask2\n",
    "\n",
    "\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     query_image, query_mask, gallery_images, gallery_masks = batch\n",
    "    #     guide_image1, guide_image2, guide_mask1, guide_mask2 = self.forward(query_image, gallery_images, gallery_masks)\n",
    "    #     composite_input = torch.cat([\n",
    "    #         query_image,\n",
    "    #         guide_image1,\n",
    "    #         guide_image2,\n",
    "    #         guide_mask1.unsqueeze(1),\n",
    "    #         guide_mask2.unsqueeze(1)\n",
    "    #     ], dim=1)\n",
    "    #     pred_masks = self.segmentation_model(composite_input)\n",
    "    #     loss = dice_coef_loss(pred_masks, query_mask, num_classes=self.num_classes)\n",
    "\n",
    "        \n",
    "    #     self.mean_valid_loss.update(loss, weight=composite_input.shape[0])\n",
    "        \n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Expects batch as a tuple:\n",
    "         - query_image: (B, 3, H, W)\n",
    "         - query_mask: (B, H, W)\n",
    "         - gallery_images: (B, N, 3, H, W)\n",
    "         - gallery_masks: (B, N, H, W)\n",
    "        \"\"\"\n",
    "        print(\"training_step\")\n",
    "        query_image, query_mask, gallery_images, gallery_masks = batch\n",
    "\n",
    "        # Retrieve the two guide images and masks\n",
    "        guide_image1, guide_image2, guide_mask1, guide_mask2 = self.forward(query_image, gallery_images, gallery_masks)\n",
    "        print(\"came out of forward\")\n",
    "        # Form composite input: concatenate along channel dimension\n",
    "        # Query image (3), guide image1 (3), guide image2 (3), guide mask1 (1), guide mask2 (1) = 11 channels total.\n",
    "        composite_input = torch.cat([\n",
    "            query_image,\n",
    "            guide_image1,\n",
    "            guide_image2,\n",
    "            guide_mask1.unsqueeze(1),\n",
    "            guide_mask2.unsqueeze(1)\n",
    "        ], dim=1)  # (B, 11, H, W)\n",
    "        \n",
    "        print(\"composite_input\")\n",
    "        # Forward pass through segmentation model\n",
    "        pred_masks = self.segmentation_model(composite_input)  # (B, num_classes, H, W)\n",
    "        print(\"pred_masks\")\n",
    "        # Compute loss using Dice + Cross-Entropy loss\n",
    "        loss = dice_coef_loss(pred_masks, query_mask, num_classes=self.num_classes)\n",
    "        print('loss')\n",
    "        self.mean_train_loss.update(loss, weight=composite_input.shape[0])\n",
    "        print('loss')\n",
    "        self.log(\"train/batch_loss\", self.mean_train_loss, prog_bar=True, logger=False)\n",
    "        print('log')\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        print(\"on_train_epoch_end\")\n",
    "        # Compute the mean training loss \n",
    "        train_loss = self.mean_train_loss.compute()\n",
    "        print(\"mean_train_loss.compute\")\n",
    "        \n",
    "        # Log the metrics for display in the progress bar and logger\n",
    "        self.log(\"train/loss\", train_loss,)\n",
    "        self.log(\"epoch\", self.current_epoch)\n",
    "\n",
    "        # Print the metrics to the console\n",
    "        print(f\"Epoch {self.current_epoch}: Train Loss: {train_loss}\") \n",
    "        \n",
    "        # Reset the metrics for the next epoch\n",
    "        self.mean_train_loss.reset()\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        print(\"validation_step\")\n",
    "        query_image, query_mask, gallery_images, gallery_masks = batch\n",
    "        guide_image1, guide_image2, guide_mask1, guide_mask2 = self.forward(query_image, gallery_images, gallery_masks)\n",
    "        composite_input = torch.cat([\n",
    "            query_image,\n",
    "            guide_image1,\n",
    "            guide_image2,\n",
    "            guide_mask1.unsqueeze(1),\n",
    "            guide_mask2.unsqueeze(1)\n",
    "        ], dim=1)\n",
    "        pred_masks = self.segmentation_model(composite_input)\n",
    "        loss = dice_coef_loss(pred_masks, query_mask, num_classes=self.num_classes)\n",
    "\n",
    "        \n",
    "        self.mean_valid_loss.update(loss, weight=composite_input.shape[0])\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        print(\"on_validation_epoch_end\")\n",
    "        # Compute the mean validation loss\n",
    "        valid_loss = self.mean_valid_loss.compute()\n",
    "        \n",
    "        \n",
    "        # Log the metrics for display in the progress bar and logger\n",
    "        self.log(\"valid/loss\", valid_loss, prog_bar=True)\n",
    "        self.log(\"epoch\", self.current_epoch)\n",
    "\n",
    "        # Print the metrics to the console\n",
    "        print(f\"Epoch {self.current_epoch}: Valid Loss: {valid_loss}\") \n",
    "        \n",
    "        # Reset the metrics for the next epoch\n",
    "        self.mean_valid_loss.reset()\n",
    "     \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            list(self.retrieval_model.parameters()) + list(self.segmentation_model.parameters()),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba630bb-b2bd-4ec1-a9a2-7b279d2d4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm import tqdm\n",
    "# Importing torchmetrics modular and functional implementations.\n",
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0704dadd-50a2-4b46-b0e7-d844884113a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_rgb(img):\n",
    "    return img.convert(\"RGB\")\n",
    "    \n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Lambda(convert_to_rgb),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1489d9ff-85b6-4e89-b805-09ce1038adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def remap_labels(target, mapping = {0: 0, 85: 1, 170: 2, 255: 3}):\n",
    "    \"\"\"\n",
    "    Remap the labels in the target tensor according to the provided mapping.\n",
    "    \n",
    "    Args:\n",
    "        target (torch.Tensor): The original target tensor with labels to be remapped.\n",
    "        mapping (dict): A dictionary mapping original labels to new labels.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The target tensor with remapped labels.\n",
    "    \"\"\"\n",
    "    remapped_target = torch.zeros_like(target, dtype=torch.long)\n",
    "    for original_label, new_label in mapping.items():\n",
    "        remapped_target[target == original_label] = new_label\n",
    "    return remapped_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aaee203-84b8-4997-b444-0894a69800f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_ed = '/home/saahmed/scratch/projects/Image-segmentation/datasets/ACDC/processed_data/Training/ed/images/'\n",
    "mask_dir_ed = '/home/saahmed/scratch/projects/Image-segmentation/datasets/ACDC/processed_data/Training/ed/masks/'\n",
    "\n",
    "\n",
    "image_dir_es = '/home/saahmed/scratch/projects/Image-segmentation/datasets/ACDC/processed_data/Training/es/images/'\n",
    "mask_dir_es = '/home/saahmed/scratch/projects/Image-segmentation/datasets/ACDC/processed_data/Training/es/masks/'\n",
    "\n",
    "\n",
    "image_filenames = sorted([os.path.join(image_dir_ed, i) for i in os.listdir(image_dir_ed)]) + \\\n",
    "                           sorted([os.path.join(image_dir_es, i) for i in os.listdir(image_dir_es)])\n",
    "mask_filenames = sorted([os.path.join(mask_dir_ed, i) for i in os.listdir(mask_dir_ed)]) + \\\n",
    "                          sorted([os.path.join(mask_dir_es, i) for i in os.listdir(mask_dir_es)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66eab6eb-b7b6-4e36-91b8-c6868f6824ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1902it [01:01, 31.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid image-mask pairs: 1841\n",
      "Number of training pairs: 1472\n",
      "Number of validation pairs: 369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n",
    "def is_mask_empty(mask):\n",
    "    \"\"\"\n",
    "    Check if a mask is empty.\n",
    "    Here, empty means all pixel values are zero.\n",
    "    \"\"\"\n",
    "    return np.all(mask == 255)\n",
    "\n",
    "\n",
    "valid_images = []\n",
    "valid_masks = []\n",
    "\n",
    "for img_path, mask_path in tqdm(zip(image_filenames,mask_filenames)):\n",
    "    if not os.path.exists(mask_path):\n",
    "        print(f\"Mask file not found for image: {img_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Read mask image as grayscale\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if mask is None:\n",
    "        print(f\"Could not read mask file: {mask_path}\")\n",
    "        continue\n",
    "    \n",
    "    # If the mask is empty, skip this image\n",
    "    if is_mask_empty(mask):\n",
    "        # print(f\"{mask_path}\\n\\n\")\n",
    "        # print(np.unique(mask))\n",
    "        continue\n",
    "    \n",
    "    valid_images.append(img_path)\n",
    "    valid_masks.append(mask_path)\n",
    "\n",
    "print(\"Total valid image-mask pairs:\", len(valid_images))\n",
    "\n",
    "# Split the valid image-mask pairs into training and validation sets\n",
    "train_imgs, val_imgs, train_masks, val_masks = train_test_split(\n",
    "    valid_images, valid_masks, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Number of training pairs:\", len(train_imgs))\n",
    "print(\"Number of validation pairs:\", len(val_imgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af44ba75-4234-4220-9dd0-2210a6976002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_imgs = train_imgs[:369]\n",
    "# train_masks = train_masks[:369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6ebda6b-2a41-4c01-adfa-3d5b4bad99dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1841it [02:01, 15.12it/s]\n"
     ]
    }
   ],
   "source": [
    "database_images = []\n",
    "database_masks = []\n",
    "\n",
    "for img , mask in tqdm(zip(valid_images,valid_masks)):\n",
    "    g_img = Image.open(img)\n",
    "    g_mask = Image.open(mask).convert('L')\n",
    "\n",
    "    processed_img = preprocess(g_img)\n",
    "    # with torch.no_grad():\n",
    "    #     processed_img_emb = retrieval_model(processed_img.unsqueeze(0).to('cuda'))\n",
    "    #     processed_img_emb = processed_img_emb.cpu()  # Move back to CPU to free up GPU memory\n",
    "\n",
    "    g_mask = TF.resize(g_mask, (256,256), interpolation=Image.NEAREST)\n",
    "    \n",
    "    g_mask = torch.from_numpy(np.array(g_mask)).long()\n",
    "    g_mask = remap_labels(g_mask)\n",
    "\n",
    "    database_images.append(processed_img)\n",
    "    database_masks.append(g_mask)\n",
    "    # database_images_embeddings.append(processed_img_emb)\n",
    "database_images = database_images[:10]\n",
    "database_masks = database_masks[:10]\n",
    "# gallery_images: (num_gallery, 3, H, W) and gallery_masks: (num_gallery, H, W)\n",
    "database_images = torch.stack(database_images, dim=0)#.unsqueeze(0)\n",
    "database_masks = torch.stack(database_masks, dim=0)#.unsqueeze(0)\n",
    "# database_images_embeddings = torch.stack(database_images_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e8d8732-977d-42aa-a0d7-4b7f61815d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class JointMedicalDataset(Dataset):\n",
    "    def __init__(self, image_file_names, mask_file_names, database_images, database_masks, image_size=(256, 256)):\n",
    "\n",
    "        self.database_images = database_images\n",
    "        self.database_masks = database_masks\n",
    "\n",
    "       \n",
    "        self.image_filenames = image_file_names\n",
    "        self.mask_filenames  = mask_file_names\n",
    "        assert len(self.image_filenames) == len(self.mask_filenames), \"Number of images and masks do not match\"\n",
    "        \n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load query image and mask\n",
    "        query_image_path = self.image_filenames[idx]\n",
    "        query_mask_path = self.mask_filenames[idx]\n",
    "\n",
    "        query_image = Image.open(query_image_path)\n",
    "        query_mask = Image.open(query_mask_path).convert('L')\n",
    "        \n",
    "        # query_image = TF.resize(query_image, self.image_size)\n",
    "        query_mask = TF.resize(query_mask, self.image_size, interpolation=Image.NEAREST)\n",
    "        \n",
    "        # query_image = TF.to_tensor(query_image)\n",
    "        # query_image = TF.normalize(query_image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        query_image = preprocess(query_image)\n",
    "        query_mask = torch.from_numpy(np.array(query_mask)).long()\n",
    "        query_mask = remap_labels(query_mask)\n",
    "\n",
    "     \n",
    "        return query_image, query_mask, database_images, database_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c75170a-0c6f-4508-b309-a9e76d32b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    DATA_TRAIN_IMAGES = train_imgs\n",
    "    DATA_TRAIN_LABELS = train_masks\n",
    "\n",
    "    DATA_VALID_IMAGES = val_imgs\n",
    "    DATA_VALID_LABELS = val_masks\n",
    "\n",
    "    Guide_database_imgs = database_images\n",
    "    Guide_database_masks = database_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c214e7d2-1acc-4259-b685-595c0e6e4b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalSegmentationDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=4,\n",
    "        img_size=(256, 256),\n",
    "        ds_mean=(0.485, 0.456, 0.406),\n",
    "        ds_std=(0.229, 0.224, 0.225),\n",
    "        batch_size=4,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        shuffle_validation=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    " \n",
    "        self.num_classes = num_classes\n",
    "        self.img_size    = img_size\n",
    "        self.ds_mean     = ds_mean\n",
    "        self.ds_std      = ds_std\n",
    "        self.batch_size  = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory  = pin_memory\n",
    "         \n",
    "        self.shuffle_validation = shuffle_validation\n",
    "\n",
    "    def setup(self, *args, **kwargs):\n",
    "        # # Create training dataset and dataloader.\n",
    "        # train_imgs_ed = sorted(glob(f\"{Paths.DATA_TRAIN_IMAGES_ED}\"))\n",
    "        # train_msks_ed  = sorted(glob(f\"{Paths.DATA_TRAIN_LABELS_ED}\"))\n",
    "\n",
    "        # train_imgs_es = sorted(glob(f\"{Paths.DATA_TRAIN_IMAGES_ES}\"))\n",
    "        # train_msks_es  = sorted(glob(f\"{Paths.DATA_TRAIN_LABELS_ES}\"))\n",
    " \n",
    "        # train_imgs = train_imgs_ed + train_imgs_es\n",
    "        # train_msks  = train_msks_ed + train_msks_es\n",
    " \n",
    "        # # Create validation dataset and dataloader.\n",
    "        # valid_imgs = train_imgs_ed + train_imgs_es\n",
    "        # valid_msks = train_msks_ed + train_msks_es\n",
    "\n",
    "        train_imgs = Paths.DATA_TRAIN_IMAGES\n",
    "        train_msks = Paths.DATA_TRAIN_LABELS\n",
    "        \n",
    "        valid_imgs =  Paths.DATA_VALID_IMAGES\n",
    "        valid_msks = Paths.DATA_VALID_LABELS\n",
    "\n",
    "        # test_imgs =  Paths.DATA_TEST_IMAGES\n",
    "        # test_msks = Paths.DATA_TEST_LABELS\n",
    "        guide_images = Paths.Guide_database_imgs\n",
    "        guide_masks = Paths.Guide_database_masks\n",
    " \n",
    "        self.train_ds =  JointMedicalDataset(train_imgs, train_msks, guide_images, guide_masks)\n",
    " \n",
    "        self.valid_ds =  JointMedicalDataset(valid_imgs, valid_msks, guide_images, guide_masks)\n",
    " \n",
    "\n",
    "        # self.test_ds = MedicalDataset(image_paths=test_imgs, mask_paths=test_msks, img_size=self.img_size, \n",
    "        #                                is_train=False, ds_mean=self.ds_mean, ds_std=self.ds_std)\n",
    " \n",
    "    def train_dataloader(self):\n",
    "        # Create train dataloader object with drop_last flag set to True.\n",
    "        return DataLoader(\n",
    "            self.train_ds, batch_size=self.batch_size,  pin_memory=self.pin_memory, \n",
    "            num_workers=self.num_workers, drop_last=True, shuffle=True\n",
    "        )    \n",
    " \n",
    "    def val_dataloader(self):\n",
    "        # Create validation dataloader object.\n",
    "        return DataLoader(\n",
    "            self.valid_ds, batch_size=self.batch_size,  pin_memory=self.pin_memory, \n",
    "            num_workers=self.num_workers, shuffle=self.shuffle_validation\n",
    "        )\n",
    "\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     # Create validation dataloader object.\n",
    "    #     return DataLoader(\n",
    "    #         self.test_ds, batch_size=self.batch_size,  pin_memory=self.pin_memory, \n",
    "    #         num_workers=self.num_workers, shuffle=self.shuffle_validation\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14e69668-1e9a-4f78-b1a1-307ddff4b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = JointMedicalDataset(train_imgs, train_masks, database_images, database_masks)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "# val_dataset = JointMedicalDataset(val_imgs, val_masks, database_images, database_masks)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc472df-ffbe-428d-b531-00ebe5d5b903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 441 μs, sys: 0 ns, total: 441 μs\n",
      "Wall time: 452 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    " \n",
    "dm = MedicalSegmentationDataModule(\n",
    "    num_classes=4,\n",
    "    img_size=(256, 256),\n",
    "    batch_size=4,\n",
    "    num_workers=0,\n",
    "    shuffle_validation=False,\n",
    ")\n",
    " \n",
    "\n",
    "# Create training & validation dataset.\n",
    "dm.setup()\n",
    " \n",
    "train_loader, valid_loader = dm.train_dataloader(), dm.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fbddb86-3cde-4c35-99bc-188489417035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1414065/58455593.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=\"cuda:0\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create the segmentation model using a pretrained segformer.\n",
    "# segformer_base = SegformerForSemanticSegmentation.from_pretrained(\n",
    "#     \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
    "#     num_labels=4,\n",
    "#     ignore_mismatched_sizes=True\n",
    "# )\n",
    "\n",
    "ckpt_path = '/scratch/saahmed/projects/Image-segmentation/segmentation/lightning_logs/version_0/checkpoints/ckpt_053-vloss_0.1769_vf1_0.9259.ckpt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cuda:0\")\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "\n",
    "# Remove any unwanted prefixes from the state_dict keys\n",
    "new_state_dict = {key.replace(\"model.\", \"\"): value for key, value in state_dict.items()}\n",
    "\n",
    "# Load the configuration from a pretrained model and update the number of classes\n",
    "config = SegformerForSemanticSegmentation.config_class.from_pretrained(\"nvidia/segformer-b4-finetuned-ade-512-512\")\n",
    "config.num_labels = 4  # Update to the number of classes used in your checkpoint\n",
    "\n",
    "# Instantiate the model with the updated configuration\n",
    "segformer_base = SegformerForSemanticSegmentation(config)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "segformer_base.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "497c2dca-6368-4e5e-bc49-aaa615e5a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_model = JointSegmentationModel(segformer_base, composite_in_channels=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a35dc414-e751-4908-bd61-58152aa48a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_module = JointTrainingModule(retrieval_model, segmentation_model ,num_classes=4, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6db559e5-8a2f-41a8-854d-17d50ced6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import TQDMProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5d2eb73-63f2-4a92-973d-40e0a18df548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Seed everything for reproducibility.\n",
    "pl.seed_everything(42, workers=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b19dd4d7-e461-4886-add6-8ab3b98f648d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cvmfs/soft.computecanada.ca/easybuild/software/2023 ...\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type                   | Params | Mode \n",
      "----------------------------------------------------------------------\n",
      "0 | retrieval_model    | SimCLR                 | 86.8 M | train\n",
      "1 | segmentation_model | JointSegmentationModel | 64.0 M | train\n",
      "2 | mean_train_loss    | MeanMetric             | 0      | train\n",
      "3 | mean_valid_loss    | MeanMetric             | 0      | train\n",
      "----------------------------------------------------------------------\n",
      "150 M     Trainable params\n",
      "0         Non-trainable params\n",
      "150 M     Total params\n",
      "603.226   Total estimated model params size (MB)\n",
      "947       Modules in train mode\n",
      "236       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad41edb764464fc58f7ed8b4d2b368a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "validation_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "on_validation_epoch_end\n",
      "Epoch 0: Valid Loss: 1.4951763153076172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838bedc24aa54c0c89bc1c1ac72c6f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n",
      "training_step\n",
      "calculated similarity\n",
      "differentiable_top2\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "einsum\n",
      "came out of forward\n",
      "composite_input\n",
      "pred_masks\n",
      "loss\n",
      "loss\n",
      "log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "[rank: 0] Received SIGTERM: 15\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1056\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:176\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/core/module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1302\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py:79\u001b[0m, in \u001b[0;36mMixedPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:241\u001b[0m, in \u001b[0;36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackward_fn\u001b[39m(loss: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:328\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 328\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:213\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[0;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpost_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py:73\u001b[0m, in \u001b[0;36mPrecision.backward\u001b[0;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/core/module.py:1097\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[0;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1097\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      2\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      3\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Auto select the best hardware accelerator available\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoint_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/saahmed.57930105.0/jupyter_new/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:65\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     64\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     68\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"auto\",  # Auto select the best hardware accelerator available\n",
    "    devices='auto',  # Auto select available devices for the accelerator (For eg. mutiple GPUs)\n",
    "    strategy=\"auto\",  # Auto select the distributed training strategy.\n",
    "    precision=\"16-mixed\",  # Using Mixed Precision training.\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=10)],\n",
    "    enable_progress_bar=True,\n",
    "log_every_n_steps=10\n",
    ")\n",
    "\n",
    "trainer.fit(joint_module, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc592e7-6b00-45b2-a123-2eca65090b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = JointMedicalDataset(num_samples=200, num_gallery=10)\n",
    "# val_dataset = DummyJointDataset(num_samples=50, num_gallery=10)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a15ab-3506-4cdb-9f12-e2b98ee925a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Example data (replace with your actual arrays)\n",
    "# two_images = np.random.rand(2, 1, 128)  # Shape (2, 1, 128)\n",
    "# many_images = np.random.rand(1902, 1, 128)  # Assuming (1902, 1, 128) for simplicity\n",
    "\n",
    "# # Reshape the arrays to remove unnecessary dimensions and get (2, 128) and (1902, 128)\n",
    "# two_images = two_images.squeeze(1)  # Shape becomes (2, 128)\n",
    "# many_images = many_images.squeeze(1)  # Shape becomes (1902, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a61a3-6282-48d4-be40-41a6d1fb759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.zeros([1902, 1, 128]).squeeze(1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c57dc3-446c-49b3-ae1f-942702d186e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.zeros([2, 1, 128]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82faa39c-2b0c-40aa-98a6-494ab7887a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Prepare DataLoaders\n",
    "# ----------------------------\n",
    "\n",
    "# train_dataset = DummyJointDataset(num_samples=200, num_gallery=10)\n",
    "# val_dataset = DummyJointDataset(num_samples=50, num_gallery=10)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "# # ----------------------------\n",
    "# # Instantiate Models\n",
    "# # ----------------------------\n",
    "\n",
    "# # Create the retrieval model (SimCLR). In your actual code, load your checkpoint if needed.\n",
    "# retrieval_model = SimCLR(dropout_p=0.3, embedding_size=128, freeze=False, linear_eval=True)\n",
    "\n",
    "# # Create the segmentation model using a pretrained segformer.\n",
    "# segformer_base = SegformerForSemanticSegmentation.from_pretrained(\n",
    "#     \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
    "#     num_labels=4,\n",
    "#     ignore_mismatched_sizes=True\n",
    "# )\n",
    "# # Wrap the segformer with an adapter to accept composite input (11 channels).\n",
    "# segmentation_model = JointSegmentationModel(segformer_base, composite_in_channels=11)\n",
    "\n",
    "# # ----------------------------\n",
    "# # Instantiate the Joint Training Module\n",
    "# # ----------------------------\n",
    "\n",
    "# joint_module = JointTrainingModule(retrieval_model, segmentation_model, num_classes=4, lr=1e-4)\n",
    "\n",
    "# ----------------------------\n",
    "# Train the Model with PyTorch Lightning\n",
    "# ----------------------------\n",
    "\n",
    "# trainer = pl.Trainer(max_epochs=10, gpus=0)  # Set gpus=1 if you have a GPU available.\n",
    "# trainer.fit(joint_module, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daac05e-cb4a-4219-8d31-1f3c6f36eb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30517d14-18a2-4d6e-b7f1-6d1e003a5c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a99d9b-f73a-4653-bb32-5719fb97e4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252862b-24a3-4482-b038-30b16c8958c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
